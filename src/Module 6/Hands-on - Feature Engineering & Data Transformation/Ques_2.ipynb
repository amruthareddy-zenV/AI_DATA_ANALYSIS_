{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data (first 5 rows):\n",
      "   CustomerID  Age  Annual_Income Region\n",
      "0           1   53   45089.846087   West\n",
      "1           2   18   54265.388660   West\n",
      "2           3   59   41984.666315  North\n",
      "3           4   64   55339.601596  South\n",
      "4           5   31   35971.049873  North\n",
      "\n",
      "Original Data Description:\n",
      "       CustomerID         Age  Annual_Income\n",
      "count  100.000000  100.000000     100.000000\n",
      "mean    50.500000   42.300000   60176.515177\n",
      "std     29.011492   12.968883   20598.013550\n",
      "min      1.000000   18.000000   10000.000000\n",
      "25%     25.750000   32.000000   44737.849244\n",
      "50%     50.500000   43.000000   61852.482997\n",
      "75%     75.250000   53.000000   74392.238432\n",
      "max    100.000000   64.000000  123689.374162\n",
      "\n",
      "Original Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   CustomerID     100 non-null    int64  \n",
      " 1   Age            100 non-null    int64  \n",
      " 2   Annual_Income  100 non-null    float64\n",
      " 3   Region         100 non-null    object \n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 3.2+ KB\n",
      "None\n",
      "\n",
      "Original DataFrame shape: (100, 4)\n",
      "\n",
      "Transformed Data (first 5 rows):\n",
      "   num__Age  num__Annual_Income  cat__Region_East  cat__Region_North  \\\n",
      "0  0.760870            0.308647               0.0                0.0   \n",
      "1  0.000000            0.389354               0.0                0.0   \n",
      "2  0.891304            0.281334               0.0                1.0   \n",
      "3  1.000000            0.398802               0.0                0.0   \n",
      "4  0.282609            0.228439               0.0                1.0   \n",
      "\n",
      "   cat__Region_South  cat__Region_West  remainder__CustomerID  \n",
      "0                0.0               1.0                    1.0  \n",
      "1                0.0               1.0                    2.0  \n",
      "2                0.0               0.0                    3.0  \n",
      "3                1.0               0.0                    4.0  \n",
      "4                0.0               0.0                    5.0  \n",
      "\n",
      "Transformed Data Description:\n",
      "         num__Age  num__Annual_Income  cat__Region_East  cat__Region_North  \\\n",
      "count  100.000000          100.000000        100.000000         100.000000   \n",
      "mean     0.528261            0.441347          0.190000           0.310000   \n",
      "std      0.281932            0.181178          0.394277           0.464823   \n",
      "min      0.000000            0.000000          0.000000           0.000000   \n",
      "25%      0.304348            0.305551          0.000000           0.000000   \n",
      "50%      0.543478            0.456089          0.000000           0.000000   \n",
      "75%      0.760870            0.566387          0.000000           1.000000   \n",
      "max      1.000000            1.000000          1.000000           1.000000   \n",
      "\n",
      "       cat__Region_South  cat__Region_West  remainder__CustomerID  \n",
      "count         100.000000        100.000000             100.000000  \n",
      "mean            0.250000          0.250000              50.500000  \n",
      "std             0.435194          0.435194              29.011492  \n",
      "min             0.000000          0.000000               1.000000  \n",
      "25%             0.000000          0.000000              25.750000  \n",
      "50%             0.000000          0.000000              50.500000  \n",
      "75%             0.250000          0.250000              75.250000  \n",
      "max             1.000000          1.000000             100.000000  \n",
      "\n",
      "Transformed Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 7 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   num__Age               100 non-null    float64\n",
      " 1   num__Annual_Income     100 non-null    float64\n",
      " 2   cat__Region_East       100 non-null    float64\n",
      " 3   cat__Region_North      100 non-null    float64\n",
      " 4   cat__Region_South      100 non-null    float64\n",
      " 5   cat__Region_West       100 non-null    float64\n",
      " 6   remainder__CustomerID  100 non-null    float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 5.6 KB\n",
      "None\n",
      "\n",
      "Transformed DataFrame shape: (100, 7)\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling & Encoding\n",
    "\n",
    "# Objective: Learn to scale numerical features and encode categorical features for better model performance.\n",
    "# Instructions:\n",
    "# For each example, perform the following steps:\n",
    "#     1. Load the Dataset: Load the dataset into your environment.\n",
    "#     2. Feature Scaling: Apply scaling methods (StandardScaler or MinMaxScaler) to specified numerical columns.\n",
    "#     3. Feature Encoding: Apply encoding methods (One-Hot Encoding or Label Encoding) to specified categorical columns.\n",
    "#     4. Verify Changes: Check the data to ensure proper scaling and encoding. \n",
    "\n",
    "\n",
    "# Task:\n",
    "#     Dataset: customer_data.csv (get it by your own it includes the columns of Age , Annual_Income)\n",
    "#     Columns to scale: Age , Annual_Income\n",
    "#     Column to encode: Region\n",
    "#     Steps:\n",
    "#         1. Load customer_data.csv .\n",
    "#         2. Use MinMaxScaler on Age and Annual_Income .\n",
    "#         3. Perform One-Hot Encoding on Region .\n",
    "#         4. Verify by assessing the transformed dataset.\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# --- Step 1: Load the Dataset ---\n",
    "# IMPORTANT: Replace this section with your actual code to load customer_data.csv\n",
    "# Example: df = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# Creating a sample DataFrame that simulates customer data with numerical and categorical features\n",
    "# This is for demonstration purposes as I cannot access local files.\n",
    "data = {\n",
    "    'CustomerID': range(1, 101),\n",
    "    'Age': np.random.randint(18, 65, size=100), # Ages between 18 and 65\n",
    "    'Annual_Income': np.random.normal(loc=60000, scale=20000, size=100), # Simulate income with some variance\n",
    "    'Region': np.random.choice(['North', 'South', 'East', 'West'], size=100, p=[0.3, 0.2, 0.25, 0.25]) # Categorical feature\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Ensure Annual_Income is positive (income cannot be negative)\n",
    "df['Annual_Income'] = df['Annual_Income'].apply(lambda x: max(x, 10000)) # Set a minimum income\n",
    "\n",
    "print(\"Original Data (first 5 rows):\")\n",
    "print(df.head())\n",
    "print(\"\\nOriginal Data Description:\")\n",
    "print(df.describe())\n",
    "print(\"\\nOriginal Data Info:\")\n",
    "print(df.info())\n",
    "print(f\"\\nOriginal DataFrame shape: {df.shape}\")\n",
    "\n",
    "# Define the columns for scaling and encoding\n",
    "numerical_features = ['Age', 'Annual_Income']\n",
    "categorical_features = ['Region']\n",
    "\n",
    "# --- Step 2: Use MinMaxScaler on Age and Annual_Income ---\n",
    "# --- Step 3: Perform One-Hot Encoding on Region ---\n",
    "\n",
    "# Create a ColumnTransformer to apply different transformations to different columns\n",
    "# This is the recommended way to handle mixed data types in scikit-learn.\n",
    "# transformers: list of tuples (name, transformer_object, columns_to_apply_to)\n",
    "# remainder='passthrough': keeps the columns not specified in transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), numerical_features), # Apply MinMaxScaler to numerical features\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features) # Apply OneHotEncoder to categorical features\n",
    "    ],\n",
    "    remainder='passthrough' # Keep other columns (like CustomerID)\n",
    ")\n",
    "\n",
    "# Create a pipeline that first preprocesses the data and then could be followed by a model\n",
    "# Using a pipeline is good practice for chaining steps.\n",
    "# Here, we just use the preprocessor step to show the transformation.\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Apply the transformations to the DataFrame\n",
    "# fit_transform fits the transformers on the data and then transforms it.\n",
    "# The output is a numpy array.\n",
    "X_transformed = pipeline.fit_transform(df)\n",
    "\n",
    "# --- Step 4: Verify by assessing the transformed dataset ---\n",
    "\n",
    "# Get the names of the new columns created by the preprocessor\n",
    "# This requires fitting the preprocessor first, which is done by the pipeline's fit_transform.\n",
    "# We need to access the preprocessor step within the fitted pipeline.\n",
    "transformed_feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out(df.columns)\n",
    "\n",
    "# Create a new DataFrame from the transformed data with appropriate column names\n",
    "df_transformed = pd.DataFrame(X_transformed, columns=transformed_feature_names)\n",
    "\n",
    "print(\"\\nTransformed Data (first 5 rows):\")\n",
    "print(df_transformed.head())\n",
    "print(\"\\nTransformed Data Description:\")\n",
    "# Describe the transformed data to see the effect of scaling and encoding\n",
    "print(df_transformed.describe())\n",
    "print(\"\\nTransformed Data Info:\")\n",
    "print(df_transformed.info())\n",
    "print(f\"\\nTransformed DataFrame shape: {df_transformed.shape}\")\n",
    "\n",
    "# --- Interpretation ---\n",
    "# Observe the 'Age' and 'Annual_Income' columns in the transformed data.\n",
    "# Their values should now be scaled between 0 and 1.\n",
    "# Observe the new columns created for 'Region' (e.g., 'cat__Region_East', 'cat__Region_North', etc.).\n",
    "# These are the one-hot encoded columns, where each row will have a 1 in the column\n",
    "# corresponding to its original region and 0 in others.\n",
    "# The 'remainder__CustomerID' column should be present and unchanged if remainder='passthrough'.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
