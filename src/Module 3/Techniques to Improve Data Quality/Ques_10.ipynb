{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'recordlinkage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrecordlinkage\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrecordlinkage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Block\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'recordlinkage'"
     ]
    }
   ],
   "source": [
    "# Question: Advanced Deduplication Using Machine Learning\n",
    "# Description: Implement ML-based deduplication based on feature similarity.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import recordlinkage\n",
    "from recordlinkage.index import Block\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MLDeduplicator:\n",
    "    \"\"\"\n",
    "    Advanced deduplication using machine learning to identify similar records.\n",
    "    \n",
    "    Features:\n",
    "    - Handles both text and numeric data\n",
    "    - Configurable similarity thresholds\n",
    "    - Multiple matching strategies\n",
    "    - Interactive duplicate review\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=5, text_similarity_threshold=0.85, numeric_similarity_threshold=0.9):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.text_sim_thresh = text_similarity_threshold\n",
    "        self.num_sim_thresh = numeric_similarity_threshold\n",
    "        \n",
    "    def _preprocess_data(self, df, text_cols, numeric_cols):\n",
    "        \"\"\"Create feature vectors for similarity comparison\"\"\"\n",
    "        # Text processing pipeline\n",
    "        text_pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words='english', analyzer='char', ngram_range=(2, 4)))\n",
    "        ])\n",
    "        \n",
    "        # Numeric processing pipeline\n",
    "        numeric_pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        # Combined preprocessor\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('text', text_pipeline, text_cols),\n",
    "            ('numeric', numeric_pipeline, numeric_cols)\n",
    "        ])\n",
    "        \n",
    "        return preprocessor.fit_transform(df)\n",
    "    \n",
    "    def _find_candidate_pairs(self, df, text_cols, numeric_cols):\n",
    "        \"\"\"Find potential duplicate pairs using blocking and nearest neighbors\"\"\"\n",
    "        # Create blocking index\n",
    "        indexer = recordlinkage.Index()\n",
    "        if text_cols:\n",
    "            indexer.add(Block(text_cols[0]))\n",
    "        if numeric_cols:\n",
    "            indexer.add(Block(numeric_cols[0]))\n",
    "        candidate_pairs = indexer.index(df)\n",
    "        \n",
    "        # Get feature vectors\n",
    "        X = self._preprocess_data(df, text_cols, numeric_cols)\n",
    "        \n",
    "        # Find nearest neighbors\n",
    "        nn = NearestNeighbors(n_neighbors=self.n_neighbors, metric='cosine')\n",
    "        nn.fit(X)\n",
    "        distances, indices = nn.kneighbors(X)\n",
    "        \n",
    "        # Create pairs from nearest neighbors\n",
    "        pairs = set()\n",
    "        for i in range(len(indices)):\n",
    "            for j, dist in zip(indices[i], distances[i]):\n",
    "                if i != j and (j, i) not in pairs:\n",
    "                    pairs.add((i, j))\n",
    "        \n",
    "        return list(pairs), X\n",
    "    \n",
    "    def _calculate_similarities(self, pairs, X, df, text_cols, numeric_cols):\n",
    "        \"\"\"Calculate detailed similarity metrics for candidate pairs\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, j in pairs:\n",
    "            # Overall feature similarity\n",
    "            feature_sim = 1 - cosine_similarity([X[i]], [X[j]])[0][0]\n",
    "            \n",
    "            # Text similarity (average of text columns)\n",
    "            text_sim = 0\n",
    "            if text_cols:\n",
    "                text_sims = []\n",
    "                for col in text_cols:\n",
    "                    vec1 = TfidfVectorizer().fit_transform([df.at[i, col]])\n",
    "                    vec2 = TfidfVectorizer().fit_transform([df.at[j, col]])\n",
    "                    text_sims.append(cosine_similarity(vec1, vec2)[0][0])\n",
    "                text_sim = np.mean(text_sims)\n",
    "            \n",
    "            # Numeric similarity (average of numeric columns)\n",
    "            num_sim = 0\n",
    "            if numeric_cols:\n",
    "                num_sims = []\n",
    "                for col in numeric_cols:\n",
    "                    val1 = df.at[i, col]\n",
    "                    val2 = df.at[j, col]\n",
    "                    if pd.notna(val1) and pd.notna(val2):\n",
    "                        num_sims.append(1 - abs(val1 - val2) / (df[col].max() - df[col].min()))\n",
    "                num_sim = np.mean(num_sims) if num_sims else 0\n",
    "            \n",
    "            results.append({\n",
    "                'id1': i,\n",
    "                'id2': j,\n",
    "                'feature_similarity': feature_sim,\n",
    "                'text_similarity': text_sim,\n",
    "                'numeric_similarity': num_sim,\n",
    "                'is_duplicate': (text_sim >= self.text_sim_thresh) and \n",
    "                               (num_sim >= self.num_sim_thresh)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def deduplicate(self, df, text_cols=None, numeric_cols=None):\n",
    "        \"\"\"\n",
    "        Identify and flag duplicate records based on feature similarity.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): Input dataframe\n",
    "            text_cols (list): List of text columns to compare\n",
    "            numeric_cols (list): List of numeric columns to compare\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Original dataframe with duplicate flags\n",
    "            pd.DataFrame: Similarity matrix of potential duplicates\n",
    "        \"\"\"\n",
    "        if text_cols is None:\n",
    "            text_cols = []\n",
    "        if numeric_cols is None:\n",
    "            numeric_cols = []\n",
    "        \n",
    "        # Find candidate pairs and preprocess data\n",
    "        pairs, X = self._find_candidate_pairs(df, text_cols, numeric_cols)\n",
    "        \n",
    "        # Calculate detailed similarities\n",
    "        similarity_df = self._calculate_similarities(pairs, X, df, text_cols, numeric_cols)\n",
    "        \n",
    "        # Mark duplicates in original dataframe\n",
    "        duplicate_ids = set()\n",
    "        for _, row in similarity_df[similarity_df['is_duplicate']].iterrows():\n",
    "            duplicate_ids.add(row['id2'])\n",
    "        \n",
    "        df['is_duplicate'] = False\n",
    "        df.loc[df.index.isin(duplicate_ids), 'is_duplicate'] = True\n",
    "        \n",
    "        return df, similarity_df\n",
    "    \n",
    "    def interactive_review(self, df, similarity_df):\n",
    "        \"\"\"Interactive tool to review and confirm duplicates\"\"\"\n",
    "        print(f\"Found {len(similarity_df)} potential duplicate pairs\")\n",
    "        print(f\"Auto-flagged {similarity_df['is_duplicate'].sum()} as duplicates\")\n",
    "        \n",
    "        # Filter for high similarity but not auto-flagged\n",
    "        review_df = similarity_df[\n",
    "            (similarity_df['feature_similarity'] > 0.7) & \n",
    "            (~similarity_df['is_duplicate'])\n",
    "        ].sort_values('feature_similarity', ascending=False)\n",
    "        \n",
    "        for _, row in review_df.iterrows():\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"Pair {row['id1']} and {row['id2']}\")\n",
    "            print(f\"Similarity Scores:\")\n",
    "            print(f\"  Feature: {row['feature_similarity']:.2f}\")\n",
    "            print(f\"  Text: {row['text_similarity']:.2f}\")\n",
    "            print(f\"  Numeric: {row['numeric_similarity']:.2f}\")\n",
    "            \n",
    "            print(\"\\nRecord 1:\")\n",
    "            print(df.iloc[row['id1']])\n",
    "            print(\"\\nRecord 2:\")\n",
    "            print(df.iloc[row['id2']])\n",
    "            \n",
    "            response = input(\"\\nAre these duplicates? (y/n/skip): \").lower()\n",
    "            if response == 'y':\n",
    "                df.at[row['id2'], 'is_duplicate'] = True\n",
    "            elif response == 'skip':\n",
    "                break\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample customer data with potential duplicates\n",
    "    data = {\n",
    "        'customer_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'name': ['John Smith', 'Jon Smith', 'John Smyth', 'Alice Johnson', 'A. Johnson', \n",
    "                'Robert Brown', 'Bob Brown', 'Robert Browning', 'Mary Wilson', 'Marie Wilson'],\n",
    "        'address': ['123 Main St', '123 Main Street', '123 Main St', '456 Oak Ave', '456 Oak Avenue',\n",
    "                   '789 Pine Rd', '789 Pine Road', '789 Pines Rd', '321 Elm Dr', '321 Elm Drive'],\n",
    "        'age': [35, 35, 36, 28, 28, 42, 41, 42, 55, 54],\n",
    "        'join_date': ['2020-01-15', '2020-01-15', '2020-01-16', '2019-05-20', '2019-05-20',\n",
    "                     '2021-02-10', '2021-02-11', '2021-02-10', '2018-11-05', '2018-11-05']\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Initialize deduplicator\n",
    "    deduper = MLDeduplicator(\n",
    "        n_neighbors=3,\n",
    "        text_similarity_threshold=0.8,\n",
    "        numeric_similarity_threshold=0.85\n",
    "    )\n",
    "    \n",
    "    # Perform deduplication\n",
    "    text_cols = ['name', 'address']\n",
    "    numeric_cols = ['age']\n",
    "    deduped_df, similarity_df = deduper.deduplicate(df, text_cols, numeric_cols)\n",
    "    \n",
    "    # Interactive review\n",
    "    final_df = deduper.interactive_review(deduped_df, similarity_df)\n",
    "    \n",
    "    # Results\n",
    "    print(\"\\nFINAL DUPLICATE COUNTS:\")\n",
    "    print(final_df['is_duplicate'].value_counts())\n",
    "    \n",
    "    print(\"\\nDUPLICATE RECORDS:\")\n",
    "    print(final_df[final_df['is_duplicate']])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
