{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gfhfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analyzing Complete Duplicates ===\n",
      "\n",
      "Loaded dataset with 8 rows and 5 columns\n",
      "First 5 rows:\n",
      "   CustomerID     Name       Email    JoinDate  PurchaseAmount\n",
      "0         101    Alice  a@test.com  2023-01-01             100\n",
      "1         102      Bob  b@test.com  2023-02-01             150\n",
      "2         103  Charlie  c@test.com  2023-03-01             200\n",
      "3         101    Alice  a@test.com  2023-01-01             100\n",
      "4         104    David  d@test.com  2023-04-01             175\n",
      "\n",
      "=== Checking for Complete Duplicate Rows ===\n",
      "\n",
      "Found 5 duplicate records (all instances shown):\n",
      "   CustomerID   Name       Email    JoinDate  PurchaseAmount\n",
      "0         101  Alice  a@test.com  2023-01-01             100\n",
      "3         101  Alice  a@test.com  2023-01-01             100\n",
      "7         101  Alice  a@test.com  2023-01-01             100\n",
      "1         102    Bob  b@test.com  2023-02-01             150\n",
      "5         102    Bob  b@test.com  2023-02-01             150\n",
      "\n",
      "Total duplicate rows (excluding first occurrence): 3\n",
      "Percentage of duplicates: 37.50%\n",
      "\n",
      "\n",
      "=== Analyzing Email Duplicates ===\n",
      "\n",
      "Loaded dataset with 8 rows and 5 columns\n",
      "First 5 rows:\n",
      "   CustomerID     Name       Email    JoinDate  PurchaseAmount\n",
      "0         101    Alice  a@test.com  2023-01-01             100\n",
      "1         102      Bob  b@test.com  2023-02-01             150\n",
      "2         103  Charlie  c@test.com  2023-03-01             200\n",
      "3         101    Alice  a@test.com  2023-01-01             100\n",
      "4         104    David  d@test.com  2023-04-01             175\n",
      "\n",
      "=== Checking for Duplicates in Columns: ['Email'] ===\n",
      "\n",
      "Found 5 duplicate records (all instances shown):\n",
      "   CustomerID   Name       Email    JoinDate  PurchaseAmount\n",
      "0         101  Alice  a@test.com  2023-01-01             100\n",
      "3         101  Alice  a@test.com  2023-01-01             100\n",
      "7         101  Alice  a@test.com  2023-01-01             100\n",
      "1         102    Bob  b@test.com  2023-02-01             150\n",
      "5         102    Bob  b@test.com  2023-02-01             150\n",
      "\n",
      "Total duplicate rows (excluding first occurrence): 3\n",
      "Percentage of duplicates: 37.50%\n",
      "\n",
      "\n",
      "=== Analyzing CustomerID Duplicates ===\n",
      "\n",
      "Loaded dataset with 8 rows and 5 columns\n",
      "First 5 rows:\n",
      "   CustomerID     Name       Email    JoinDate  PurchaseAmount\n",
      "0         101    Alice  a@test.com  2023-01-01             100\n",
      "1         102      Bob  b@test.com  2023-02-01             150\n",
      "2         103  Charlie  c@test.com  2023-03-01             200\n",
      "3         101    Alice  a@test.com  2023-01-01             100\n",
      "4         104    David  d@test.com  2023-04-01             175\n",
      "\n",
      "=== Checking for Duplicates in Columns: ['CustomerID'] ===\n",
      "\n",
      "Found 5 duplicate records (all instances shown):\n",
      "   CustomerID   Name       Email    JoinDate  PurchaseAmount\n",
      "0         101  Alice  a@test.com  2023-01-01             100\n",
      "3         101  Alice  a@test.com  2023-01-01             100\n",
      "7         101  Alice  a@test.com  2023-01-01             100\n",
      "1         102    Bob  b@test.com  2023-02-01             150\n",
      "5         102    Bob  b@test.com  2023-02-01             150\n",
      "\n",
      "Total duplicate rows (excluding first occurrence): 3\n",
      "Percentage of duplicates: 37.50%\n"
     ]
    }
   ],
   "source": [
    "# Activity 2: Dealing with Duplicates & Redundancy\n",
    "\n",
    "# Task A: Identifying Duplicate Records\n",
    "\n",
    "# 7. Identify Complete Duplicates:\n",
    "# - Load a dataset and identify duplicated rows.\n",
    "# - Use Pandas to detect duplicates.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 8. Identify Duplicates based on Specific Columns:\n",
    "# - Check for duplicates in specified columns.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 9. Count Duplicate Rows:\n",
    "# - Calculate and print the number of duplicate rows.\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_analyze_duplicates(file_path, duplicate_columns=None):\n",
    "    \"\"\"\n",
    "    Analyze duplicates in a dataset with options for complete or column-specific checks\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the dataset file\n",
    "        duplicate_columns (list): Columns to check for duplicates (None for complete rows)\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"\\nLoaded dataset with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    print(\"First 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # 7. Identify Complete Duplicates\n",
    "    if duplicate_columns is None:\n",
    "        print(\"\\n=== Checking for Complete Duplicate Rows ===\")\n",
    "        duplicates = df[df.duplicated(keep=False)]  # keep=False marks all duplicates\n",
    "    else:\n",
    "        print(f\"\\n=== Checking for Duplicates in Columns: {duplicate_columns} ===\")\n",
    "        duplicates = df[df.duplicated(subset=duplicate_columns, keep=False)]\n",
    "    \n",
    "    # 8. Display duplicate records\n",
    "    if len(duplicates) > 0:\n",
    "        print(f\"\\nFound {len(duplicates)} duplicate records (all instances shown):\")\n",
    "        print(duplicates.sort_values(duplicate_columns if duplicate_columns else df.columns.tolist()))\n",
    "    else:\n",
    "        print(\"\\nNo duplicates found based on the specified criteria\")\n",
    "    \n",
    "    # 9. Count duplicate rows\n",
    "    if duplicate_columns is None:\n",
    "        duplicate_count = df.duplicated().sum()\n",
    "    else:\n",
    "        duplicate_count = df.duplicated(subset=duplicate_columns).sum()\n",
    "    \n",
    "    print(f\"\\nTotal duplicate rows (excluding first occurrence): {duplicate_count}\")\n",
    "    print(f\"Percentage of duplicates: {duplicate_count/len(df):.2%}\")\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample data (in practice, use your actual file path)\n",
    "    data = {\n",
    "        'CustomerID': [101, 102, 103, 101, 104, 102, 105, 101],\n",
    "        'Name': ['Alice', 'Bob', 'Charlie', 'Alice', 'David', 'Bob', 'Eve', 'Alice'],\n",
    "        'Email': ['a@test.com', 'b@test.com', 'c@test.com', 'a@test.com', \n",
    "                 'd@test.com', 'b@test.com', 'e@test.com', 'a@test.com'],\n",
    "        'JoinDate': ['2023-01-01', '2023-02-01', '2023-03-01', '2023-01-01',\n",
    "                    '2023-04-01', '2023-02-01', '2023-05-01', '2023-01-01'],\n",
    "        'PurchaseAmount': [100, 150, 200, 100, 175, 150, 225, 100]\n",
    "    }\n",
    "    sample_df = pd.DataFrame(data)\n",
    "    sample_df.to_csv('customer_data.csv', index=False)\n",
    "    \n",
    "    print(\"=== Analyzing Complete Duplicates ===\")\n",
    "    complete_dupes = load_and_analyze_duplicates('customer_data.csv')\n",
    "    \n",
    "    print(\"\\n\\n=== Analyzing Email Duplicates ===\")\n",
    "    email_dupes = load_and_analyze_duplicates('customer_data.csv', duplicate_columns=['Email'])\n",
    "    \n",
    "    print(\"\\n\\n=== Analyzing CustomerID Duplicates ===\")\n",
    "    id_dupes = load_and_analyze_duplicates('customer_data.csv', duplicate_columns=['CustomerID'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original dataset shape: (8, 5)\n",
      "Sample data:\n",
      "   CustomerID     Name       Email    JoinDate  PurchaseAmount\n",
      "0         101    Alice  a@test.com  2023-01-01             100\n",
      "1         102      Bob  b@test.com  2023-02-01             150\n",
      "2         103  Charlie  c@test.com  2023-03-01             200\n",
      "3         101    Alice  a@test.com  2023-01-15             120\n",
      "4         104    David  d@test.com  2023-04-01             175\n",
      "\n",
      "=== 10. Complete Deduplication (Keep First) ===\n",
      "Removed 0 duplicate rows\n",
      "Resulting dataset shape: (8, 5)\n",
      "Saved as 'deduplicated_first.csv'\n",
      "\n",
      "=== 11. Subset Deduplication (Columns: Email) ===\n",
      "Removed 2 duplicate rows based on columns: ['Email']\n",
      "Resulting dataset shape: (6, 5)\n",
      "Saved as 'deduplicated_subset.csv'\n",
      "\n",
      "=== 12. Complete Deduplication (Keep Last) ===\n",
      "Removed 0 duplicate rows\n",
      "Resulting dataset shape: (8, 5)\n",
      "\n",
      "Comparison of first vs last occurrence keeping:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Saved as 'deduplicated_last.csv'\n"
     ]
    }
   ],
   "source": [
    "# Task B: Deduplication Techniques\n",
    "\n",
    "# 10. Remove Complete Duplicates:\n",
    "# - Drop duplicate rows and keep only the first occurrence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 11. Subset Deduplication:\n",
    "# - Remove duplicates based on a subset of columns.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 12. Keep Last Occurrence:\n",
    "# - Drop duplicates but keep the last occurrence in the dataset.\n",
    "import pandas as pd\n",
    "\n",
    "def demonstrate_deduplication(file_path):\n",
    "    \"\"\"\n",
    "    Demonstrate various deduplication techniques on a dataset\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the dataset file\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"\\nOriginal dataset shape: {df.shape}\")\n",
    "    print(\"Sample data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # 10. Remove Complete Duplicates (keep first)\n",
    "    df_first = df.drop_duplicates()\n",
    "    print(\"\\n=== 10. Complete Deduplication (Keep First) ===\")\n",
    "    print(f\"Removed {len(df) - len(df_first)} duplicate rows\")\n",
    "    print(f\"Resulting dataset shape: {df_first.shape}\")\n",
    "    df_first.to_csv('deduplicated_first.csv', index=False)\n",
    "    print(\"Saved as 'deduplicated_first.csv'\")\n",
    "    \n",
    "    # 11. Subset Deduplication (example: based on 'Email' column)\n",
    "    subset_cols = ['Email']  # Change to your desired columns\n",
    "    df_subset = df.drop_duplicates(subset=subset_cols)\n",
    "    print(\"\\n=== 11. Subset Deduplication (Columns: {}) ===\".format(', '.join(subset_cols)))\n",
    "    print(f\"Removed {len(df) - len(df_subset)} duplicate rows based on columns: {subset_cols}\")\n",
    "    print(f\"Resulting dataset shape: {df_subset.shape}\")\n",
    "    df_subset.to_csv('deduplicated_subset.csv', index=False)\n",
    "    print(\"Saved as 'deduplicated_subset.csv'\")\n",
    "    \n",
    "    # 12. Keep Last Occurrence\n",
    "    df_last = df.drop_duplicates(keep='last')\n",
    "    print(\"\\n=== 12. Complete Deduplication (Keep Last) ===\")\n",
    "    print(f\"Removed {len(df) - len(df_last)} duplicate rows\")\n",
    "    print(f\"Resulting dataset shape: {df_last.shape}\")\n",
    "    print(\"\\nComparison of first vs last occurrence keeping:\")\n",
    "    print(pd.concat([\n",
    "        df[df.duplicated(keep=False)].groupby(list(df.columns)).first(),\n",
    "        df[df.duplicated(keep=False)].groupby(list(df.columns)).last()\n",
    "    ], axis=1, keys=['First Occurrence', 'Last Occurrence']))\n",
    "    df_last.to_csv('deduplicated_last.csv', index=False)\n",
    "    print(\"\\nSaved as 'deduplicated_last.csv'\")\n",
    "    \n",
    "    return df_first, df_subset, df_last\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample data (in practice, use your actual file path)\n",
    "    data = {\n",
    "        'CustomerID': [101, 102, 103, 101, 104, 102, 105, 101],\n",
    "        'Name': ['Alice', 'Bob', 'Charlie', 'Alice', 'David', 'Bob', 'Eve', 'Alice'],\n",
    "        'Email': ['a@test.com', 'b@test.com', 'c@test.com', 'a@test.com', \n",
    "                 'd@test.com', 'b@test.com', 'e@test.com', 'a_new@test.com'],\n",
    "        'JoinDate': ['2023-01-01', '2023-02-01', '2023-03-01', '2023-01-15',\n",
    "                    '2023-04-01', '2023-02-15', '2023-05-01', '2023-01-20'],\n",
    "        'PurchaseAmount': [100, 150, 200, 120, 175, 160, 225, 110]\n",
    "    }\n",
    "    sample_df = pd.DataFrame(data)\n",
    "    sample_df.to_csv('customer_data.csv', index=False)\n",
    "    \n",
    "    # Run deduplication demonstrations\n",
    "    df_first, df_subset, df_last = demonstrate_deduplication('customer_data.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
