{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'great_expectations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgx\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpectations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgxe\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'great_expectations'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import great_expectations as gx\n",
    "import great_expectations.expectations as gxe\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- Activity 4: Data Quality Automation Tools ---\n",
    "# --- Task A: Using Great Expectations ---\n",
    "\n",
    "# --- 19. Setting Up Expectations & Validation ---\n",
    "# Objective: Install GE, set up expectation suite, validate, list unmet expectations.\n",
    "\n",
    "# 1. Create a sample dataset (using pandas DataFrame)\n",
    "print(\"1. Creating a sample dataset...\")\n",
    "data = {\n",
    "    'product_id': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
    "    'product_name': ['Laptop', 'Keyboard', 'Mouse', 'Monitor', 'Webcam', 'Printer', 'Scanner', 'Speakers', 'Headphones', 'Microphone'],\n",
    "    'price': [1200.50, 75.00, 25.99, 350.75, 80.00, 250.00, 150.00, 100.00, 99.50, 70.00],\n",
    "    'in_stock': [True, True, False, True, True, False, True, True, True, True],\n",
    "    'rating': [4.5, 4.0, 3.0, 4.8, 4.2, 3.5, 4.1, 4.3, 4.6, 4.0],\n",
    "    'last_updated': ['2023-01-15', '2024-02-20', '2023-11-10', '2024-03-01', '2024-04-05', '2023-09-30', '2024-01-25', '2024-03-15', '2024-04-20', '2024-05-01']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce some data quality issues for demonstration\n",
    "df.loc[2, 'price'] = -10.0 # Invalid price (outside range)\n",
    "df.loc[5, 'in_stock'] = None # Missing value\n",
    "df.loc[8, 'rating'] = 5.5 # Invalid rating (outside range)\n",
    "df.loc[0, 'product_id'] = 102 # Duplicate product_id\n",
    "\n",
    "print(\"Sample DataFrame created with some data quality issues:\")\n",
    "print(df)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 2. Set up a basic Great Expectations Data Context\n",
    "# In a real-world scenario, you would typically run `great_expectations init`\n",
    "# in your terminal to set up the project structure.\n",
    "# For this script, we'll create a temporary file-based context to support Data Docs.\n",
    "\n",
    "# Define a temporary GE project directory\n",
    "ge_root_dir = \"gx_temp_project\"\n",
    "\n",
    "# Clean up previous runs if the directory exists\n",
    "if os.path.exists(ge_root_dir):\n",
    "    print(f\"Removing existing GE project directory: {ge_root_dir}\")\n",
    "    shutil.rmtree(ge_root_dir)\n",
    "\n",
    "print(f\"Initializing Great Expectations Data Context in '{ge_root_dir}'...\")\n",
    "# Initialize the context - this simulates `great_expectations init`\n",
    "# We use a simple file-based configuration\n",
    "context = gx.get_context(project_root_dir=ge_root_dir)\n",
    "\n",
    "# Add a simple datasource (e.g., for pandas DataFrames)\n",
    "# This simulates configuring a datasource via `great_expectations add datasource`\n",
    "print(\"Adding a datasource...\")\n",
    "datasource_name = \"my_pandas_datasource\"\n",
    "try:\n",
    "    context.add_datasource(\n",
    "        name=datasource_name,\n",
    "        module_name=\"great_expectations.datasource\",\n",
    "        class_name=\"Datasource\",\n",
    "        execution_engine={\n",
    "            \"module_name\": \"great_expectations.execution_engine\",\n",
    "            \"class_name\": \"PandasExecutionEngine\",\n",
    "        },\n",
    "        data_connectors={\n",
    "            \"default_runtime_data_connector\": {\n",
    "                \"class_name\": \"RuntimeDataConnector\",\n",
    "                \"module_name\": \"great_expectations.datasource.data_connector\",\n",
    "                \"batch_identifiers\":[\"batch_id\"] # Required for RuntimeDataConnector\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    print(f\"Datasource '{datasource_name}' added.\")\n",
    "except gx.exceptions.DataContextError as e:\n",
    "    print(f\"Datasource '{datasource_name}' already exists or error adding: {e}\")\n",
    "    # If it already exists, retrieve it\n",
    "    context.get_datasource(datasource_name)\n",
    "\n",
    "\n",
    "# 3. Create an Expectation Suite\n",
    "# This simulates creating an expectation suite, e.g., via `great_expectations suite new`\n",
    "print(\"Creating an Expectation Suite...\")\n",
    "suite_name = \"product_data_quality_suite\"\n",
    "\n",
    "# Check if suite already exists, if so, load it\n",
    "try:\n",
    "    suite = context.get_expectation_suite(expectation_suite_name=suite_name)\n",
    "    print(f\"Expectation Suite '{suite_name}' loaded.\")\n",
    "except gx.exceptions.DataContextError:\n",
    "    suite = context.create_expectation_suite(expectation_suite_name=suite_name)\n",
    "    print(f\"Expectation Suite '{suite_name}' created.\")\n",
    "\n",
    "# --- 20. Testing for Expectation (Adding Expectations) ---\n",
    "# Objective: Create expectations like \"column values must fall within a certain range.\"\n",
    "\n",
    "print(\"Adding expectations to the suite...\")\n",
    "\n",
    "# Add expectations based on the dataset and potential quality issues\n",
    "\n",
    "# Expectation: 'product_id' column values must be unique\n",
    "suite.add_expectation(gxe.ExpectationConfiguration(\n",
    "    expectation_type=\"expect_column_values_to_be_unique\",\n",
    "    kwargs={\"column\": \"product_id\"}\n",
    "))\n",
    "\n",
    "# Expectation: 'product_name' column values must not be null\n",
    "suite.add_expectation(gxe.ExpectationConfiguration(\n",
    "    expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "    kwargs={\"column\": \"product_name\"}\n",
    "))\n",
    "\n",
    "# Expectation: 'price' column values must be between 0 and 10000 (Task 20 - Range Expectation)\n",
    "suite.add_expectation(gxe.ExpectationConfiguration(\n",
    "    expectation_type=\"expect_column_values_to_be_between\",\n",
    "    kwargs={\"column\": \"price\", \"min_value\": 0, \"max_value\": 10000}\n",
    "))\n",
    "\n",
    "# Expectation: 'in_stock' column values must be boolean\n",
    "suite.add_expectation(gxe.ExpectationConfiguration(\n",
    "    expectation_type=\"expect_column_values_to_be_boolean\",\n",
    "    kwargs={\"column\": \"in_stock\"}\n",
    "))\n",
    "\n",
    "# Expectation: 'rating' column values must be between 0 and 5 (Task 20 - Another Range Expectation)\n",
    "suite.add_expectation(gxe.ExpectationConfiguration(\n",
    "    expectation_type=\"expect_column_values_to_be_between\",\n",
    "    kwargs={\"column\": \"rating\", \"min_value\": 0, \"max_value\": 5}\n",
    "))\n",
    "\n",
    "# Expectation: 'last_updated' column values must be in a date format\n",
    "suite.add_expectation(gxe.ExpectationConfiguration(\n",
    "    expectation_type=\"expect_column_values_to_match_regex\",\n",
    "    kwargs={\"column\": \"last_updated\", \"regex\": r\"^\\d{4}-\\d{2}-\\d{2}$\"}\n",
    "))\n",
    "\n",
    "\n",
    "# Save the Expectation Suite\n",
    "# This saves the suite definition to the great_expectations/expectations directory\n",
    "context.save_expectation_suite(expectation_suite=suite, expectation_suite_name=suite_name)\n",
    "print(f\"Expectation Suite '{suite_name}' saved.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 4. Validate the dataset against the expectation suite (Task 19 - Validate a dataset)\n",
    "\n",
    "print(\"Validating the dataset against the expectation suite...\")\n",
    "\n",
    "# Create a Batch Request for the DataFrame\n",
    "# We use a RuntimeBatchRequest because our data is already in a DataFrame\n",
    "batch_request = gx.BatchRequest(\n",
    "    datasource_name=datasource_name,\n",
    "    data_connector_name=\"default_runtime_data_connector\",\n",
    "    data_asset_name=\"my_product_data\", # A name for this specific data asset\n",
    "    runtime_parameters={\"batch_data\": df}, # Pass the DataFrame here\n",
    "    batch_identifiers={\"batch_id\": \"first_batch\"} # A unique identifier for this batch\n",
    ")\n",
    "\n",
    "# Create and run a Checkpoint\n",
    "# Checkpoints are the primary way to validate data in GE\n",
    "checkpoint_name = \"my_product_checkpoint\"\n",
    "checkpoint_config = {\n",
    "    \"name\": checkpoint_name,\n",
    "    \"class_name\": \"SimpleCheckpoint\",\n",
    "    \"validations\": [\n",
    "        {\n",
    "            \"batch_request\": batch_request,\n",
    "            \"expectation_suite_name\": suite_name,\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Add the checkpoint to the context\n",
    "try:\n",
    "    context.add_checkpoint(**checkpoint_config)\n",
    "    print(f\"Checkpoint '{checkpoint_name}' added.\")\n",
    "except gx.exceptions.DataContextError as e:\n",
    "     print(f\"Checkpoint '{checkpoint_name}' already exists or error adding: {e}\")\n",
    "     # If it exists, retrieve it\n",
    "     context.get_checkpoint(checkpoint_name)\n",
    "\n",
    "\n",
    "# Run the checkpoint\n",
    "checkpoint_result = context.run_checkpoint(checkpoint_name=checkpoint_name)\n",
    "\n",
    "print(\"\\nValidation Complete.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 5. List unmet expectations (Task 19 - List unmet expectations)\n",
    "\n",
    "print(\"Listing unmet expectations:\")\n",
    "\n",
    "# Check the validation result\n",
    "if not checkpoint_result[\"success\"]:\n",
    "    print(\"Validation failed. Unmet expectations:\")\n",
    "    # Iterate through the results to find failed expectations\n",
    "    for validation_result_identifier, validation_result in checkpoint_result[\"run_results\"].items():\n",
    "        # Access the results of the validation\n",
    "        results = validation_result['validation_result']['results']\n",
    "        for result in results:\n",
    "            if not result['success']:\n",
    "                expectation = result['expectation_config']\n",
    "                print(f\"- Expectation Type: {expectation['expectation_type']}\")\n",
    "                print(f\"  Column: {expectation['kwargs'].get('column', 'N/A')}\")\n",
    "                print(f\"  Details: {result['result']}\")\n",
    "                print(f\"  Meta: {result['meta']}\")\n",
    "else:\n",
    "    print(\"Validation successful! All expectations met.\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 21. Generating Data Docs ---\n",
    "# Objective: Automatically generate data quality documentation.\n",
    "\n",
    "print(\"Attempting to build Data Docs...\")\n",
    "\n",
    "# To build Data Docs, you typically need the full GE project structure\n",
    "# created by `great_expectations init`.\n",
    "# The context.build_data_docs() command generates the HTML documentation.\n",
    "# You can then open the index.html file in your browser to view the docs.\n",
    "\n",
    "try:\n",
    "    # Build Data Docs\n",
    "    docs_build_results = context.build_data_docs()\n",
    "    print(\"\\nData Docs built successfully.\")\n",
    "    # The URL to open will be printed in the console output of the GE command.\n",
    "    # In this script, we'll just indicate where they are.\n",
    "    data_docs_path = os.path.join(ge_root_dir, 'uncommitted', 'data_docs', 'local_site')\n",
    "    print(f\"Data Docs are located in: {os.path.abspath(data_docs_path)}\")\n",
    "    print(\"Open the 'index.html' file in that directory in your web browser to view them.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not build Data Docs. This might require a more complete GE project setup.\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure you have run 'great_expectations init' or have a valid GE project structure.\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"Script finished.\")\n",
    "\n",
    "# Optional: Clean up the temporary GE project directory\n",
    "# print(f\"Cleaning up temporary GE project directory: {ge_root_dir}\")\n",
    "# shutil.rmtree(ge_root_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original DataFrame ---\n",
      "   CustomerID ProductName   Price  Quantity   OrderDate\n",
      "0         101      Laptop  1200.0       1.0  2023-01-10\n",
      "1         102    Keyboard    75.0       2.0  2023-01-11\n",
      "2         103       Mouse     NaN       1.0  2023-01-11\n",
      "3         104     Monitor   300.0       1.0  2023-01-12\n",
      "4         105      Webcam    50.0       3.0  2023-01-12\n",
      "5         101      Laptop  1200.0       1.0  2023-01-10\n",
      "6         106     Printer   250.0       1.0  2023-01-13\n",
      "7         107       Mouse    75.0       2.0  2023-01-11\n",
      "8         108    Keyboard    75.0       2.0  2023-01-11\n",
      "9         109     Monitor   300.0       NaN  2023-01-12\n",
      "\n",
      "\n",
      "--- Missing Values Count per Column ---\n",
      "CustomerID     0\n",
      "ProductName    0\n",
      "Price          1\n",
      "Quantity       1\n",
      "OrderDate      0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "--- Missing Values in 'Price' column ---\n",
      "1\n",
      "\n",
      "\n",
      "--- Rows with any Missing Value ---\n",
      "   CustomerID ProductName  Price  Quantity   OrderDate\n",
      "2         103       Mouse    NaN       1.0  2023-01-11\n",
      "9         109     Monitor  300.0       NaN  2023-01-12\n",
      "\n",
      "\n",
      "--- DataFrame after Dropping Rows with Missing Values ---\n",
      "   CustomerID ProductName   Price  Quantity   OrderDate\n",
      "0         101      Laptop  1200.0       1.0  2023-01-10\n",
      "1         102    Keyboard    75.0       2.0  2023-01-11\n",
      "3         104     Monitor   300.0       1.0  2023-01-12\n",
      "4         105      Webcam    50.0       3.0  2023-01-12\n",
      "5         101      Laptop  1200.0       1.0  2023-01-10\n",
      "6         106     Printer   250.0       1.0  2023-01-13\n",
      "7         107       Mouse    75.0       2.0  2023-01-11\n",
      "8         108    Keyboard    75.0       2.0  2023-01-11\n",
      "\n",
      "\n",
      "--- DataFrame after Filling Missing Values ---\n",
      "   CustomerID ProductName        Price  Quantity   OrderDate\n",
      "0         101      Laptop  1200.000000       1.0  2023-01-10\n",
      "1         102    Keyboard    75.000000       2.0  2023-01-11\n",
      "2         103       Mouse   391.666667       1.0  2023-01-11\n",
      "3         104     Monitor   300.000000       1.0  2023-01-12\n",
      "4         105      Webcam    50.000000       3.0  2023-01-12\n",
      "5         101      Laptop  1200.000000       1.0  2023-01-10\n",
      "6         106     Printer   250.000000       1.0  2023-01-13\n",
      "7         107       Mouse    75.000000       2.0  2023-01-11\n",
      "8         108    Keyboard    75.000000       2.0  2023-01-11\n",
      "9         109     Monitor   300.000000       1.0  2023-01-12\n",
      "\n",
      "\n",
      "--- Check for Exact Duplicate Rows ---\n",
      "1\n",
      "\n",
      "\n",
      "--- Identified Duplicate Rows ---\n",
      "   CustomerID ProductName   Price  Quantity   OrderDate\n",
      "0         101      Laptop  1200.0       1.0  2023-01-10\n",
      "5         101      Laptop  1200.0       1.0  2023-01-10\n",
      "\n",
      "\n",
      "--- DataFrame after Dropping Duplicate Rows ---\n",
      "   CustomerID ProductName        Price  Quantity   OrderDate\n",
      "0         101      Laptop  1200.000000       1.0  2023-01-10\n",
      "1         102    Keyboard    75.000000       2.0  2023-01-11\n",
      "2         103       Mouse   391.666667       1.0  2023-01-11\n",
      "3         104     Monitor   300.000000       1.0  2023-01-12\n",
      "4         105      Webcam    50.000000       3.0  2023-01-12\n",
      "6         106     Printer   250.000000       1.0  2023-01-13\n",
      "7         107       Mouse    75.000000       2.0  2023-01-11\n",
      "8         108    Keyboard    75.000000       2.0  2023-01-11\n",
      "9         109     Monitor   300.000000       1.0  2023-01-12\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1795/798775531.py:57: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Price'].fillna(mean_price, inplace=True)\n",
      "/tmp/ipykernel_1795/798775531.py:60: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Quantity'].fillna(1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Introduction ---\n",
    "# This script demonstrates basic data quality checks using the pandas library in Python.\n",
    "# We will cover:\n",
    "# 1. Loading data (using a sample DataFrame)\n",
    "# 2. Checking for missing values\n",
    "# 3. Handling missing values (filling or dropping)\n",
    "# 4. Checking for duplicate rows\n",
    "# 5. Identifying and handling duplicates\n",
    "\n",
    "# --- 1. Loading Data ---\n",
    "# Create a sample DataFrame for demonstration purposes\n",
    "data = {\n",
    "    'CustomerID': [101, 102, 103, 104, 105, 101, 106, 107, 108, 109],\n",
    "    'ProductName': ['Laptop', 'Keyboard', 'Mouse', 'Monitor', 'Webcam', 'Laptop', 'Printer', 'Mouse', 'Keyboard', 'Monitor'],\n",
    "    'Price': [1200, 75, np.nan, 300, 50, 1200, 250, 75, 75, 300],\n",
    "    'Quantity': [1, 2, 1, 1, 3, 1, 1, 2, 2, np.nan],\n",
    "    'OrderDate': ['2023-01-10', '2023-01-11', '2023-01-11', '2023-01-12', '2023-01-12', '2023-01-10', '2023-01-13', '2023-01-11', '2023-01-11', '2023-01-12']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"--- Original DataFrame ---\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 2. Checking for Missing Values ---\n",
    "# Check for missing values in the entire DataFrame\n",
    "print(\"--- Missing Values Count per Column ---\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check for missing values in specific columns\n",
    "print(\"--- Missing Values in 'Price' column ---\")\n",
    "print(df['Price'].isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display rows with missing values\n",
    "print(\"--- Rows with any Missing Value ---\")\n",
    "print(df[df.isnull().any(axis=1)])\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 3. Handling Missing Values ---\n",
    "\n",
    "# Option A: Dropping rows with missing values\n",
    "# Note: This is often not ideal as it can lead to data loss.\n",
    "df_dropped = df.dropna()\n",
    "print(\"--- DataFrame after Dropping Rows with Missing Values ---\")\n",
    "print(df_dropped)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Option B: Filling missing values\n",
    "# Fill missing 'Price' with the mean price\n",
    "mean_price = df['Price'].mean()\n",
    "df['Price'].fillna(mean_price, inplace=True)\n",
    "\n",
    "# Fill missing 'Quantity' with a default value (e.g., 1)\n",
    "df['Quantity'].fillna(1, inplace=True)\n",
    "\n",
    "print(\"--- DataFrame after Filling Missing Values ---\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 4. Checking for Duplicate Rows ---\n",
    "# Check for exact duplicate rows\n",
    "print(\"--- Check for Exact Duplicate Rows ---\")\n",
    "print(df.duplicated().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 5. Identifying and Handling Duplicates ---\n",
    "\n",
    "# Identify duplicate rows\n",
    "print(\"--- Identified Duplicate Rows ---\")\n",
    "# `keep=False` marks all occurrences of a duplicate as True\n",
    "print(df[df.duplicated(keep=False)])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Handle duplicates: Drop duplicate rows (keeping the first occurrence by default)\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(\"--- DataFrame after Dropping Duplicate Rows ---\")\n",
    "print(df_no_duplicates)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Conclusion ---\n",
    "# This script provided a basic overview of checking and handling missing values\n",
    "# and duplicates using pandas. More advanced data quality techniques exist\n",
    "# for specific scenarios.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
