{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Sample DataFrame ---\n",
      "   ProductID ProductName     Category    Price  StockQuantity  ReleaseDate  \\\n",
      "0        101      Laptop  Electronics  1200.50             10   2023-01-10   \n",
      "1        102    Keyboard  Electronics    75.00             50   2023-01-11   \n",
      "2        103       Mouse  Electronics    25.99              0   2023-01-11   \n",
      "3        104     Monitor  Electronics   300.00             15   2023-01-12   \n",
      "4        105      Webcam  Electronics   -50.00             25   2023-01-12   \n",
      "5        106     Printer  Electronics   250.00              5   2023-01-13   \n",
      "6        107     Speaker        Audio   150.00             12   2023-01-13   \n",
      "7        108  Headphones          NaN    99.50             30   2023-01-14   \n",
      "8        109  Microphone        Audio    70.00              8   2023-01-14   \n",
      "9        110      Router      Network    80.00             20  InvalidDate   \n",
      "\n",
      "   IsActive  \n",
      "0      True  \n",
      "1     False  \n",
      "2      True  \n",
      "3      True  \n",
      "4     False  \n",
      "5      True  \n",
      "6      True  \n",
      "7     False  \n",
      "8      True  \n",
      "9      True  \n",
      "\n",
      "\n",
      "--- Applying Validation Checks ---\n",
      "--- Data Validation Report ---\n",
      "Validation Failed: The following data quality issues were detected:\n",
      "- Column 'Category': Contains 1 missing values, but is required.\n",
      "- Column 'Price': Value range violations: Contains 1 values below the minimum (0.0).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Introduction ---\n",
    "# This script demonstrates setting up basic automated data validation\n",
    "# using pure Python and the pandas library.\n",
    "# It covers checks for missing values, data types, and value ranges.\n",
    "#\n",
    "# We will:\n",
    "# 1. Create a sample DataFrame with some data quality issues.\n",
    "# 2. Define validation functions for different types of checks.\n",
    "# 3. Apply these validation functions to the DataFrame.\n",
    "# 4. Report the validation results.\n",
    "\n",
    "# --- 1. Create a Sample DataFrame ---\n",
    "print(\"--- Creating Sample DataFrame ---\")\n",
    "data = {\n",
    "    'ProductID': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
    "    'ProductName': ['Laptop', 'Keyboard', 'Mouse', 'Monitor', 'Webcam', 'Printer', 'Speaker', 'Headphones', 'Microphone', 'Router'],\n",
    "    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Audio', np.nan, 'Audio', 'Network'], # Missing value\n",
    "    'Price': [1200.50, 75.00, 25.99, 300.00, -50.00, 250.00, 150.00, 99.50, 70.00, 80.00], # Negative price\n",
    "    'StockQuantity': [10, 50, 0, 15, 25, 5, 12, 30, 8, 20],\n",
    "    'ReleaseDate': ['2023-01-10', '2023-01-11', '2023-01-11', '2023-01-12', '2023-01-12', '2023-01-13', '2023-01-13', '2023-01-14', '2023-01-14', 'InvalidDate'], # Invalid date format\n",
    "    'IsActive': [True, False, True, True, False, True, True, False, True, True]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 2. Define Validation Functions ---\n",
    "\n",
    "def check_missing_values(df, column_name, is_required=True):\n",
    "    \"\"\"Checks for missing values in a specified column.\"\"\"\n",
    "    if is_required and df[column_name].isnull().any():\n",
    "        null_count = df[column_name].isnull().sum()\n",
    "        return f\"Column '{column_name}': Contains {null_count} missing values, but is required.\"\n",
    "    elif df[column_name].isnull().any() and not is_required:\n",
    "         # Optional columns can have missing values, maybe just report count\n",
    "         # null_count = df[column_name].isnull().sum()\n",
    "         # return f\"Column '{column_name}': Contains {null_count} missing values (optional column).\"\n",
    "         pass # No violation if not required\n",
    "    return None\n",
    "\n",
    "def check_data_type(df, column_name, expected_dtype):\n",
    "    \"\"\"Checks if a column's data type matches the expected type.\"\"\"\n",
    "    # Pandas dtypes can be tricky, especially with NaNs.\n",
    "    # This is a basic check. More robust checks might involve type casting.\n",
    "    if df[column_name].dtype != expected_dtype:\n",
    "        # Allow numeric types to be checked against compatible numeric dtypes\n",
    "        if not (pd.api.types.is_numeric_dtype(df[column_name].dtype) and pd.api.types.is_numeric_dtype(expected_dtype)):\n",
    "             return f\"Column '{column_name}': Incorrect data type. Expected '{expected_dtype}', got '{df[column_name].dtype}'.\"\n",
    "    return None\n",
    "\n",
    "def check_value_range(df, column_name, min_value=None, max_value=None):\n",
    "    \"\"\"Checks if values in a numerical column are within a specified range.\"\"\"\n",
    "    if not pd.api.types.is_numeric_dtype(df[column_name].dtype):\n",
    "        return f\"Column '{column_name}': Cannot perform range check on non-numeric type '{df[column_name].dtype}'.\"\n",
    "\n",
    "    violations = []\n",
    "    if min_value is not None:\n",
    "        # Check for values below the minimum, ignoring NaNs\n",
    "        if (df[column_name].dropna() < min_value).any():\n",
    "            invalid_count = (df[column_name].dropna() < min_value).sum()\n",
    "            violations.append(f\"Contains {invalid_count} values below the minimum ({min_value}).\")\n",
    "\n",
    "    if max_value is not None:\n",
    "         # Check for values above the maximum, ignoring NaNs\n",
    "         if (df[column_name].dropna() > max_value).any():\n",
    "             invalid_count = (df[column_name].dropna() > max_value).sum()\n",
    "             violations.append(f\"Contains {invalid_count} values above the maximum ({max_value}).\")\n",
    "\n",
    "    if violations:\n",
    "        return f\"Column '{column_name}': Value range violations: {', '.join(violations)}\"\n",
    "    return None\n",
    "\n",
    "def check_allowed_values(df, column_name, allowed_list):\n",
    "    \"\"\"Checks if values in a column are within a list of allowed values.\"\"\"\n",
    "    # Check if non-null unique values are in the allowed list\n",
    "    invalid_values = df[column_name].dropna()[~df[column_name].dropna().isin(allowed_list)].unique()\n",
    "    if invalid_values.size > 0:\n",
    "        return f\"Column '{column_name}': Contains invalid values: {list(invalid_values)}\"\n",
    "    return None\n",
    "\n",
    "# --- 3. Apply Validation ---\n",
    "print(\"--- Applying Validation Checks ---\")\n",
    "\n",
    "validation_issues = []\n",
    "\n",
    "# Define checks to apply\n",
    "# Format: (check_function, column_name, *args)\n",
    "checks_to_run = [\n",
    "    (check_missing_values, 'ProductID', True), # ProductID required\n",
    "    (check_data_type, 'ProductID', 'int64'),\n",
    "    (check_missing_values, 'ProductName', True), # ProductName required\n",
    "    (check_data_type, 'ProductName', 'object'),\n",
    "    (check_missing_values, 'Category', True), # Category required\n",
    "    (check_data_type, 'Category', 'object'),\n",
    "    (check_missing_values, 'Price', True), # Price required\n",
    "    (check_data_type, 'Price', 'float64'),\n",
    "    (check_value_range, 'Price', 0.0, None), # Price must be >= 0\n",
    "    (check_missing_values, 'StockQuantity', True), # StockQuantity required\n",
    "    (check_data_type, 'StockQuantity', 'int64'),\n",
    "    (check_value_range, 'StockQuantity', 0, None), # StockQuantity must be >= 0\n",
    "    (check_missing_values, 'ReleaseDate', True), # ReleaseDate required\n",
    "    (check_data_type, 'ReleaseDate', 'object'), # Basic type check, format check below\n",
    "    # Note: Date format validation requires more specific logic (e.g., using pd.to_datetime with format)\n",
    "    # Example (conceptual):\n",
    "    # (check_date_format, 'ReleaseDate', '%Y-%m-%d'),\n",
    "    (check_missing_values, 'IsActive', True), # IsActive required\n",
    "    (check_data_type, 'IsActive', 'bool'),\n",
    "    (check_allowed_values, 'Category', ['Electronics', 'Audio', 'Accessories', 'Software', 'Network']), # Allowed categories\n",
    "]\n",
    "\n",
    "# Run checks\n",
    "for check_func, col_name, *args in checks_to_run:\n",
    "    # Ensure column exists before checking\n",
    "    if col_name not in df.columns:\n",
    "        if args and args[0] is True: # Check if it was a required column check\n",
    "             validation_issues.append(f\"Required Column Missing: Column '{col_name}' is defined in checks but not found in DataFrame.\")\n",
    "        continue # Skip checks for non-existent columns\n",
    "\n",
    "    issue = check_func(df, col_name, *args)\n",
    "    if issue:\n",
    "        validation_issues.append(issue)\n",
    "\n",
    "# --- 4. Report Validation Results ---\n",
    "print(\"--- Data Validation Report ---\")\n",
    "\n",
    "if not validation_issues:\n",
    "    print(\"Validation Successful: No issues found based on defined checks.\")\n",
    "else:\n",
    "    print(\"Validation Failed: The following data quality issues were detected:\")\n",
    "    for issue in validation_issues:\n",
    "        print(f\"- {issue}\")\n",
    "\n",
    "# --- Conclusion ---\n",
    "# This script provides a basic framework for automated data validation\n",
    "# using pandas. You can extend this by adding more validation functions\n",
    "# and defining comprehensive checks_to_run for your specific dataset.\n",
    "# For more complex scenarios, dedicated data validation libraries are recommended.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Sample DataFrame ---\n",
      "Original DataFrame:\n",
      "    DataPointID  Measurement\n",
      "0             1          101\n",
      "1             2          105\n",
      "2             3           98\n",
      "3             4          103\n",
      "4             5           99\n",
      "5             6          110\n",
      "6             7           95\n",
      "7             8          102\n",
      "8             9          108\n",
      "9            10           97\n",
      "10           11          550\n",
      "11           12          104\n",
      "12           13          106\n",
      "13           14           96\n",
      "14           15          600\n",
      "\n",
      "\n",
      "--- Calculating Z-scores ---\n",
      "DataFrame with Z-scores:\n",
      "    DataPointID  Measurement   Z_Score\n",
      "0             1          101 -0.396719\n",
      "1             2          105 -0.371898\n",
      "2             3           98 -0.415335\n",
      "3             4          103 -0.384309\n",
      "4             5           99 -0.409129\n",
      "5             6          110 -0.340872\n",
      "6             7           95 -0.433950\n",
      "7             8          102 -0.390514\n",
      "8             9          108 -0.353283\n",
      "9            10           97 -0.421540\n",
      "10           11          550  2.389415\n",
      "11           12          104 -0.378103\n",
      "12           13          106 -0.365693\n",
      "13           14           96 -0.427745\n",
      "14           15          600  2.699675\n",
      "\n",
      "\n",
      "--- Defining Z-score Threshold: 3 ---\n",
      "Identified Outliers (where |Z_Score| > 3):\n",
      "Empty DataFrame\n",
      "Columns: [DataPointID, Measurement, Z_Score]\n",
      "Index: []\n",
      "\n",
      "\n",
      "--- Handling Outliers (Replacing with NaN) ---\n",
      "DataFrame after replacing outliers with NaN:\n",
      "    DataPointID  Measurement\n",
      "0             1        101.0\n",
      "1             2        105.0\n",
      "2             3         98.0\n",
      "3             4        103.0\n",
      "4             5         99.0\n",
      "5             6        110.0\n",
      "6             7         95.0\n",
      "7             8        102.0\n",
      "8             9        108.0\n",
      "9            10         97.0\n",
      "10           11        550.0\n",
      "11           12        104.0\n",
      "12           13        106.0\n",
      "13           14         96.0\n",
      "14           15        600.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 2: Removing Outliers by Rescaling\n",
    "# Description: Remove outliers by standardizing a numerical column using z-scores.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore # SciPy's zscore function is convenient\n",
    "\n",
    "# --- Introduction ---\n",
    "# This script demonstrates how to identify and handle outliers in a\n",
    "# numerical column of a pandas DataFrame using the z-score method.\n",
    "# Outliers are typically defined as values with a z-score above a certain threshold.\n",
    "#\n",
    "# We will:\n",
    "# 1. Create a sample DataFrame with a numerical column containing outliers.\n",
    "# 2. Calculate the z-score for each value in the column.\n",
    "# 3. Define a z-score threshold.\n",
    "# 4. Identify values that exceed the threshold (outliers).\n",
    "# 5. Handle the identified outliers (e.g., replace with NaN).\n",
    "# 6. Show the DataFrame before and after handling outliers.\n",
    "\n",
    "# --- 1. Create a Sample DataFrame ---\n",
    "print(\"--- Creating Sample DataFrame ---\")\n",
    "# Create a column with mostly values around 100, but a few much larger values (outliers)\n",
    "data = {\n",
    "    'DataPointID': range(1, 16),\n",
    "    'Measurement': [\n",
    "        101, 105, 98, 103, 99, 110, 95, 102, 108, 97,\n",
    "        550,  # Outlier 1\n",
    "        104, 106, 96,\n",
    "        600   # Outlier 2\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 2. Calculate Z-scores ---\n",
    "# Calculate the z-score for each value in the 'Measurement' column.\n",
    "# The z-score measures how many standard deviations away from the mean a data point is.\n",
    "print(\"--- Calculating Z-scores ---\")\n",
    "\n",
    "# Calculate z-scores using pandas and numpy:\n",
    "# mean = df['Measurement'].mean()\n",
    "# std_dev = df['Measurement'].std()\n",
    "# df['Z_Score'] = (df['Measurement'] - mean) / std_dev\n",
    "\n",
    "# Or using SciPy's zscore function (handles NaNs if present, but our data has none initially)\n",
    "df['Z_Score'] = zscore(df['Measurement'])\n",
    "\n",
    "print(\"DataFrame with Z-scores:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 3. Define Z-score Threshold ---\n",
    "# A common threshold for identifying outliers is an absolute z-score of 2 or 3.\n",
    "# Values with |Z_Score| > threshold are considered outliers.\n",
    "z_score_threshold = 3\n",
    "\n",
    "print(f\"--- Defining Z-score Threshold: {z_score_threshold} ---\")\n",
    "\n",
    "# --- 4. Identify Outliers ---\n",
    "# Identify rows where the absolute Z_Score is greater than the threshold.\n",
    "outliers = df[abs(df['Z_Score']) > z_score_threshold]\n",
    "\n",
    "print(f\"Identified Outliers (where |Z_Score| > {z_score_threshold}):\")\n",
    "print(outliers)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 5. Handle Outliers ---\n",
    "# There are several ways to handle outliers:\n",
    "# A) Remove the rows containing outliers.\n",
    "# B) Replace the outlier values with a specific value (e.g., NaN, mean, median).\n",
    "# C) Cap the outlier values at a certain limit.\n",
    "\n",
    "# Option B: Replace outliers with NaN (a common approach)\n",
    "print(\"--- Handling Outliers (Replacing with NaN) ---\")\n",
    "\n",
    "# Create a copy to avoid modifying the original DataFrame directly if you need it later\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Replace values in 'Measurement' column where the absolute Z_Score is > threshold with NaN\n",
    "df_cleaned.loc[abs(df_cleaned['Z_Score']) > z_score_threshold, 'Measurement'] = np.nan\n",
    "\n",
    "# Drop the 'Z_Score' column from the cleaned DataFrame as it's temporary\n",
    "df_cleaned = df_cleaned.drop(columns=['Z_Score'])\n",
    "\n",
    "print(\"DataFrame after replacing outliers with NaN:\")\n",
    "print(df_cleaned)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Option A: Remove rows with outliers (alternative handling)\n",
    "# print(\"--- Alternative Handling (Removing Rows with Outliers) ---\")\n",
    "# df_rows_removed = df[abs(df['Z_Score']) <= z_score_threshold].drop(columns=['Z_Score'])\n",
    "# print(\"DataFrame after removing rows with outliers:\")\n",
    "# print(df_rows_removed)\n",
    "# print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Conclusion ---\n",
    "# The script successfully identified values with high z-scores as outliers\n",
    "# and demonstrated how to handle them by replacing them with NaN.\n",
    "# The choice of z-score threshold depends on the data and the specific problem.\n",
    "# Z-score assumes the data is roughly normally distributed; for non-normal data,\n",
    "# other methods like IQR (Interquartile Range) might be more appropriate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Sample DataFrame ---\n",
      "Original DataFrame:\n",
      "   PersonID     Name   Age      City\n",
      "0         1    Alice  25.0  New York\n",
      "1         2      Bob  32.0    London\n",
      "2         3  Charlie   NaN     Paris\n",
      "3         4    David  41.0     Tokyo\n",
      "4         5      Eve  29.0    Sydney\n",
      "5         6    Frank  35.0  New York\n",
      "6         7    Grace   NaN    London\n",
      "7         8    Heidi  27.0     Paris\n",
      "8         9     Ivan  38.0     Tokyo\n",
      "9        10     Judy  30.0    Sydney\n",
      "\n",
      "\n",
      "Data types before conversion:\n",
      "PersonID      int64\n",
      "Name         object\n",
      "Age         float64\n",
      "City         object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "--- Filling Missing Values in 'Age' Column ---\n",
      "Missing values in 'Age' before filling: 2\n",
      "Missing values in 'Age' after filling: 0\n",
      "DataFrame after filling missing 'Age' values (using median):\n",
      "   PersonID     Name   Age      City\n",
      "0         1    Alice  25.0  New York\n",
      "1         2      Bob  32.0    London\n",
      "2         3  Charlie  31.0     Paris\n",
      "3         4    David  41.0     Tokyo\n",
      "4         5      Eve  29.0    Sydney\n",
      "5         6    Frank  35.0  New York\n",
      "6         7    Grace  31.0    London\n",
      "7         8    Heidi  27.0     Paris\n",
      "8         9     Ivan  38.0     Tokyo\n",
      "9        10     Judy  30.0    Sydney\n",
      "\n",
      "\n",
      "--- Converting 'Age' Column to Integer Type ---\n",
      "Successfully converted 'Age' to integer.\n",
      "\n",
      "Data types after conversion:\n",
      "PersonID     int64\n",
      "Name        object\n",
      "Age          int64\n",
      "City        object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "DataFrame after converting 'Age' to integer:\n",
      "   PersonID     Name  Age      City\n",
      "0         1    Alice   25  New York\n",
      "1         2      Bob   32    London\n",
      "2         3  Charlie   31     Paris\n",
      "3         4    David   41     Tokyo\n",
      "4         5      Eve   29    Sydney\n",
      "5         6    Frank   35  New York\n",
      "6         7    Grace   31    London\n",
      "7         8    Heidi   27     Paris\n",
      "8         9     Ivan   38     Tokyo\n",
      "9        10     Judy   30    Sydney\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6626/161348.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Age'].fillna(median_age, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Question 3: Applying Data Type Conversion\n",
    "# Description: Convert the 'Age' column to integers after filling missing values.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Introduction ---\n",
    "# This script demonstrates how to handle missing values in a numerical column\n",
    "# (specifically 'Age') and then convert the column to an integer data type\n",
    "# using the pandas library.\n",
    "#\n",
    "# We will:\n",
    "# 1. Create a sample DataFrame with an 'Age' column containing missing values.\n",
    "# 2. Identify and fill the missing values.\n",
    "# 3. Convert the 'Age' column to an integer type.\n",
    "# 4. Show the DataFrame before and after these operations.\n",
    "\n",
    "# --- 1. Create a Sample DataFrame ---\n",
    "print(\"--- Creating Sample DataFrame ---\")\n",
    "data = {\n",
    "    'PersonID': range(1, 11),\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Heidi', 'Ivan', 'Judy'],\n",
    "    'Age': [25, 32, np.nan, 41, 29, 35, np.nan, 27, 38, 30], # Introduce missing values (NaN)\n",
    "    'City': ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', 'New York', 'London', 'Paris', 'Tokyo', 'Sydney']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Data types before conversion:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 2. Identify and Fill Missing Values in 'Age' ---\n",
    "print(\"--- Filling Missing Values in 'Age' Column ---\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in 'Age' before filling:\", df['Age'].isnull().sum())\n",
    "\n",
    "# Option A: Fill missing values with the mean age\n",
    "# mean_age = df['Age'].mean()\n",
    "# df['Age'].fillna(mean_age, inplace=True)\n",
    "\n",
    "# Option B: Fill missing values with the median age (often more robust to outliers)\n",
    "median_age = df['Age'].median()\n",
    "df['Age'].fillna(median_age, inplace=True)\n",
    "\n",
    "print(\"Missing values in 'Age' after filling:\", df['Age'].isnull().sum())\n",
    "print(\"DataFrame after filling missing 'Age' values (using median):\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 3. Convert the 'Age' Column to Integer Type ---\n",
    "print(\"--- Converting 'Age' Column to Integer Type ---\")\n",
    "\n",
    "# Convert the column to integer.\n",
    "# Use .astype(int) or .astype('int64') or .astype('int32').\n",
    "# Note: This step requires that there are no NaN values left,\n",
    "# as pandas integer types by default do not support NaNs.\n",
    "# If you might have NaNs you want to preserve, use nullable integer types like 'Int64'.\n",
    "try:\n",
    "    df['Age'] = df['Age'].astype(int)\n",
    "    print(\"Successfully converted 'Age' to integer.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error converting 'Age' to integer: {e}\")\n",
    "    print(\"This might happen if there are still non-integer values or NaNs.\")\n",
    "    print(\"Consider using a nullable integer type like 'Int64' if NaNs should be preserved.\")\n",
    "    # Example using nullable integer type:\n",
    "    # df['Age'] = df['Age'].astype('Int64')\n",
    "\n",
    "\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"DataFrame after converting 'Age' to integer:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Conclusion ---\n",
    "# The script demonstrated filling missing values in the 'Age' column\n",
    "# and then converting it to an integer data type.\n",
    "# Ensure missing values are handled appropriately before converting to\n",
    "# non-nullable integer types.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Sample DataFrame with Issues ---\n",
      "Original DataFrame:\n",
      "   Employee ID Employee Name Department  Salary (USD)   Hire_Date\n",
      "0          101         Alice      Sales       60000.0  2022-01-15\n",
      "1          102           Bob         IT       75000.0  2021-08-20\n",
      "2          103       Charlie         IT       70000.0  2022-03-10\n",
      "3          104         David  Marketing       65000.0  2023-01-01\n",
      "4          105           Eve        NaN       80000.0  2022-06-18\n",
      "5          101         Alice      Sales       60000.0  2022-01-15\n",
      "6          106         Frank         HR       72000.0  2023-02-01\n",
      "7          107         Grace         IT           NaN  2021-11-11\n",
      "8          108         Heidi  Marketing       68000.0  2022-09-25\n",
      "9          109          Ivan      Sales       62000.0  2023-03-10\n",
      "\n",
      "\n",
      "Original Column Names: ['Employee ID', 'Employee Name', 'Department', 'Salary (USD)', 'Hire_Date']\n",
      "\n",
      "\n",
      "--- Applying the Automated Cleaning Function ---\n",
      "Standardizing column names...\n",
      "Column names standardized.\n",
      "Filling missing values using strategy: 'median'...\n",
      "Missing value filling complete.\n",
      "Removing duplicate rows...\n",
      "Removed 1 duplicate rows.\n",
      "\n",
      "Data cleaning process finished.\n",
      "\n",
      "--- Cleaned DataFrame ---\n",
      "   employee_id employee_name department  salary_usd   hire_date\n",
      "0          101         Alice      Sales     60000.0  2022-01-15\n",
      "1          102           Bob         IT     75000.0  2021-08-20\n",
      "2          103       Charlie         IT     70000.0  2022-03-10\n",
      "3          104         David  Marketing     65000.0  2023-01-01\n",
      "4          105           Eve        NaN     80000.0  2022-06-18\n",
      "6          106         Frank         HR     72000.0  2023-02-01\n",
      "7          107         Grace         IT         NaN  2021-11-11\n",
      "8          108         Heidi  Marketing     68000.0  2022-09-25\n",
      "9          109          Ivan      Sales     62000.0  2023-03-10\n",
      "\n",
      "\n",
      "Cleaned Column Names: ['employee_id', 'employee_name', 'department', 'salary_usd', 'hire_date']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 4: Automating Data Cleaning with Functions\n",
    "# Description: Create a function that automates the process of filling missing values, removing duplicates, and standardizing column names.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Introduction ---\n",
    "# This script defines a Python function to automate common data cleaning tasks:\n",
    "# 1. Standardizing column names (e.g., to lowercase with underscores).\n",
    "# 2. Filling missing values using a specified strategy.\n",
    "# 3. Removing duplicate rows.\n",
    "#\n",
    "# We will:\n",
    "# 1. Create a sample DataFrame with data quality issues.\n",
    "# 2. Define the automated cleaning function.\n",
    "# 3. Apply the function to the sample DataFrame.\n",
    "# 4. Show the DataFrame before and after cleaning.\n",
    "\n",
    "# --- 1. Create a Sample DataFrame ---\n",
    "print(\"--- Creating Sample DataFrame with Issues ---\")\n",
    "data = {\n",
    "    'Employee ID': [101, 102, 103, 104, 105, 101, 106, 107, 108, 109], # Duplicate ID\n",
    "    'Employee Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Alice', 'Frank', 'Grace', 'Heidi', 'Ivan'],\n",
    "    'Department': ['Sales', 'IT', 'IT', 'Marketing', np.nan, 'Sales', 'HR', 'IT', 'Marketing', 'Sales'], # Missing value\n",
    "    'Salary (USD)': [60000, 75000, 70000, 65000, 80000, 60000, 72000, np.nan, 68000, 62000], # Missing value\n",
    "    'Hire_Date': ['2022-01-15', '2021-08-20', '2022-03-10', '2023-01-01', '2022-06-18', '2022-01-15', '2023-02-01', '2021-11-11', '2022-09-25', '2023-03-10']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Original Column Names:\", df.columns.tolist())\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 2. Define the Automated Cleaning Function ---\n",
    "\n",
    "def clean_dataframe(df, fill_strategy='median', fill_value=None, subset_fill=None, remove_duplicates=True, standardize_cols=True):\n",
    "    \"\"\"\n",
    "    Automates common data cleaning tasks on a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to clean.\n",
    "        fill_strategy (str): Strategy for filling missing values ('mean', 'median', 'mode', 'constant', None).\n",
    "                             If None, missing values are not filled.\n",
    "        fill_value: The constant value to use if fill_strategy is 'constant'.\n",
    "        subset_fill (list): List of column names to apply the fill strategy to.\n",
    "                            If None, applies to all columns of appropriate type.\n",
    "        remove_duplicates (bool): Whether to remove duplicate rows.\n",
    "        standardize_cols (bool): Whether to standardize column names (lowercase, underscores).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    cleaned_df = df.copy() # Work on a copy to avoid modifying the original DataFrame\n",
    "\n",
    "    # 1. Standardize Column Names\n",
    "    if standardize_cols:\n",
    "        print(\"Standardizing column names...\")\n",
    "        # Convert to lowercase and replace spaces/special characters with underscores\n",
    "        cleaned_df.columns = cleaned_df.columns.str.lower().str.replace('[^a-z0-9]+', '_', regex=True).str.strip('_')\n",
    "        print(\"Column names standardized.\")\n",
    "\n",
    "    # 2. Fill Missing Values\n",
    "    if fill_strategy is not None:\n",
    "        print(f\"Filling missing values using strategy: '{fill_strategy}'...\")\n",
    "        cols_to_fill = subset_fill if subset_fill is not None else cleaned_df.columns\n",
    "\n",
    "        for col in cols_to_fill:\n",
    "            if col in cleaned_df.columns and cleaned_df[col].isnull().any():\n",
    "                if fill_strategy == 'mean' and pd.api.types.is_numeric_dtype(cleaned_df[col].dtype):\n",
    "                    fill_val = cleaned_df[col].mean()\n",
    "                    cleaned_df[col].fillna(fill_val, inplace=True)\n",
    "                    print(f\"  Filled '{col}' with mean ({fill_val:.2f}).\")\n",
    "                elif fill_strategy == 'median' and pd.api.types.is_numeric_dtype(cleaned_df[col].dtype):\n",
    "                    fill_val = cleaned_df[col].median()\n",
    "                    cleaned_df[col].fillna(fill_val, inplace=True)\n",
    "                    print(f\"  Filled '{col}' with median ({fill_val:.2f}).\")\n",
    "                elif fill_strategy == 'mode':\n",
    "                     # Mode might return multiple values; take the first\n",
    "                     mode_val = cleaned_df[col].mode()\n",
    "                     if not mode_val.empty:\n",
    "                         fill_val = mode_val[0]\n",
    "                         cleaned_df[col].fillna(fill_val, inplace=True)\n",
    "                         print(f\"  Filled '{col}' with mode ({fill_val}).\")\n",
    "                     else:\n",
    "                         print(f\"  Could not find a mode for '{col}' to fill NaNs.\")\n",
    "                elif fill_strategy == 'constant' and fill_value is not None:\n",
    "                    cleaned_df[col].fillna(fill_value, inplace=True)\n",
    "                    print(f\"  Filled '{col}' with constant value ({fill_value}).\")\n",
    "                elif fill_strategy == 'constant' and fill_value is None:\n",
    "                    print(f\"  Warning: fill_strategy is 'constant' for '{col}', but fill_value is None. Skipping fill for this column.\")\n",
    "                else:\n",
    "                    # Handle cases where strategy doesn't match dtype or is unsupported\n",
    "                    if fill_strategy in ['mean', 'median'] and not pd.api.types.is_numeric_dtype(cleaned_df[col].dtype):\n",
    "                         print(f\"  Skipping fill for '{col}': Strategy '{fill_strategy}' requires numeric data, but column type is '{cleaned_df[col].dtype}'.\")\n",
    "                    elif fill_strategy not in ['mean', 'median', 'mode', 'constant']:\n",
    "                         print(f\"  Skipping fill for '{col}': Unsupported fill_strategy '{fill_strategy}'.\")\n",
    "                    # If subset_fill was used, and the column wasn't in the original df, this would also skip\n",
    "                    # but the initial check 'if col in cleaned_df.columns' handles that.\n",
    "\n",
    "        print(\"Missing value filling complete.\")\n",
    "\n",
    "    # 3. Remove Duplicate Rows\n",
    "    if remove_duplicates:\n",
    "        print(\"Removing duplicate rows...\")\n",
    "        initial_rows = len(cleaned_df)\n",
    "        cleaned_df.drop_duplicates(inplace=True)\n",
    "        rows_after_removal = len(cleaned_df)\n",
    "        duplicates_removed = initial_rows - rows_after_removal\n",
    "        print(f\"Removed {duplicates_removed} duplicate rows.\")\n",
    "\n",
    "    print(\"\\nData cleaning process finished.\")\n",
    "    return cleaned_df\n",
    "\n",
    "# --- 3. Apply the Cleaning Function ---\n",
    "print(\"--- Applying the Automated Cleaning Function ---\")\n",
    "\n",
    "# Example usage:\n",
    "# - Fill missing values with median for numeric columns\n",
    "# - Remove duplicates\n",
    "# - Standardize column names\n",
    "cleaned_df = clean_dataframe(\n",
    "    df,\n",
    "    fill_strategy='median',\n",
    "    subset_fill=['Department', 'Salary (USD)'], # Specify columns to attempt filling\n",
    "    remove_duplicates=True,\n",
    "    standardize_cols=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- Cleaned DataFrame ---\")\n",
    "print(cleaned_df)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Cleaned Column Names:\", cleaned_df.columns.tolist())\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Conclusion ---\n",
    "# The `clean_dataframe` function successfully standardized column names,\n",
    "# filled missing values in the specified columns using the median strategy,\n",
    "# and removed the duplicate row.\n",
    "# You can customize the parameters to suit different cleaning needs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Sample DataFrame ---\n",
      "Original DataFrame:\n",
      "   DataPointID  Feature_Value\n",
      "0            1             10\n",
      "1            2             25\n",
      "2            3              5\n",
      "3            4             40\n",
      "4            5             15\n",
      "5            6             30\n",
      "6            7              8\n",
      "7            8             35\n",
      "8            9             20\n",
      "9           10             45\n",
      "\n",
      "\n",
      "--- Normalizing 'Feature_Value' to the range [0, 1] ---\n",
      "Original Min: 5\n",
      "Original Max: 45\n",
      "\n",
      "DataFrame after Min-Max Normalization:\n",
      "   DataPointID  Feature_Value  Feature_Value_Normalized\n",
      "0            1             10                     0.125\n",
      "1            2             25                     0.500\n",
      "2            3              5                     0.000\n",
      "3            4             40                     0.875\n",
      "4            5             15                     0.250\n",
      "5            6             30                     0.625\n",
      "6            7              8                     0.075\n",
      "7            8             35                     0.750\n",
      "8            9             20                     0.375\n",
      "9           10             45                     1.000\n",
      "\n",
      "\n",
      "Min of Normalized Feature: 0.0\n",
      "Max of Normalized Feature: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Question 5: Complex Data Normalization\n",
    "# Description: Normalize a numeric column to a range using min-max scaling.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Introduction ---\n",
    "# This script demonstrates how to perform min-max scaling (normalization)\n",
    "# on a numerical column in a pandas DataFrame.\n",
    "# Min-max scaling transforms data to a specified range, typically [0, 1].\n",
    "# The formula is: X_normalized = (X - X_min) / (X_max - X_min)\n",
    "# To scale to a different range [a, b]: X_scaled = a + (X - X_min) * (b - a) / (X_max - X_min)\n",
    "#\n",
    "# We will:\n",
    "# 1. Create a sample DataFrame with a numerical column.\n",
    "# 2. Define the target range for normalization (e.g., [0, 1]).\n",
    "# 3. Calculate the minimum and maximum values of the original column.\n",
    "# 4. Apply the min-max scaling formula to normalize the column.\n",
    "# 5. Show the DataFrame before and after normalization.\n",
    "\n",
    "# --- 1. Create a Sample DataFrame ---\n",
    "print(\"--- Creating Sample DataFrame ---\")\n",
    "data = {\n",
    "    'DataPointID': range(1, 11),\n",
    "    'Feature_Value': [10, 25, 5, 40, 15, 30, 8, 35, 20, 45]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 2. Define the Target Range ---\n",
    "# We will normalize the 'Feature_Value' column to the range [0, 1].\n",
    "target_min = 0\n",
    "target_max = 1\n",
    "\n",
    "print(f\"--- Normalizing 'Feature_Value' to the range [{target_min}, {target_max}] ---\")\n",
    "\n",
    "# --- 3. Calculate Original Min and Max ---\n",
    "original_min = df['Feature_Value'].min()\n",
    "original_max = df['Feature_Value'].max()\n",
    "\n",
    "print(f\"Original Min: {original_min}\")\n",
    "print(f\"Original Max: {original_max}\\n\")\n",
    "\n",
    "# Handle the case where original_max and original_min are the same\n",
    "# (i.e., all values in the column are identical) to avoid division by zero.\n",
    "if original_max == original_min:\n",
    "    print(\"Warning: All values in the column are identical. Normalization will result in a constant value.\")\n",
    "    df['Feature_Value_Normalized'] = target_min # Or any value within the target range\n",
    "else:\n",
    "    # --- 4. Apply Min-Max Scaling Formula ---\n",
    "    # X_scaled = a + (X - X_min) * (b - a) / (X_max - X_min)\n",
    "    # Here, a = target_min, b = target_max, X = df['Feature_Value']\n",
    "    df['Feature_Value_Normalized'] = target_min + (df['Feature_Value'] - original_min) * (target_max - target_min) / (original_max - original_min)\n",
    "\n",
    "# --- 5. Show the DataFrame After Normalization ---\n",
    "print(\"DataFrame after Min-Max Normalization:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Verify the min and max of the normalized column\n",
    "print(\"Min of Normalized Feature:\", df['Feature_Value_Normalized'].min())\n",
    "print(\"Max of Normalized Feature:\", df['Feature_Value_Normalized'].max())\n",
    "\n",
    "# --- Conclusion ---\n",
    "# The 'Feature_Value' column has been successfully scaled to the range [0, 1].\n",
    "# Min-max scaling is sensitive to outliers, as they will affect the calculated\n",
    "# min and max values, potentially compressing the range of the majority of data points.\n",
    "# Consider outlier handling before applying min-max scaling if necessary.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
