{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Understanding and Defining Data Quality Metrics\n",
    "**Description**: Learn how to define basic data quality metrics such as completeness, validity, and uniqueness for a simple dataset.\n",
    "\n",
    "**Steps**:\n",
    "1. Dataset: Use a CSV with columns like Name , Email , Age .\n",
    "2. Metric Definitions:\n",
    "    - Completeness: Percentage of non-null values.\n",
    "    - Validity: % of email fields containing @ .\n",
    "    - Uniqueness: Count distinct entries in the Email column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness (%):\n",
      "Name     75.0\n",
      "Email    75.0\n",
      "Age      75.0\n",
      "dtype: float64\n",
      "\n",
      "Validity (%):\n",
      "75.0\n",
      "\n",
      "Uniqueness (count):\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', None],\n",
    "    'Email': ['alice@example.com', 'bob@example.com', None, 'charlie@example.com'],\n",
    "    'Age': [25, 30, None, 35]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Completeness: Percentage of non-null values for each column\n",
    "completeness = df.notnull().mean() * 100\n",
    "\n",
    "# Validity: Percentage of email fields containing '@'\n",
    "validity = (df['Email'].str.contains('@', na=False).mean()) * 100\n",
    "\n",
    "# Uniqueness: Count of distinct entries in the Email column\n",
    "uniqueness = df['Email'].nunique()\n",
    "\n",
    "# Display metrics\n",
    "print(\"Completeness (%):\")\n",
    "print(completeness)\n",
    "print(\"\\nValidity (%):\")\n",
    "print(validity)\n",
    "print(\"\\nUniqueness (count):\")\n",
    "print(uniqueness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Calculating Data Quality Score\n",
    "**Description**: Aggregate multiple metrics to calculate an overall data quality score.\n",
    "\n",
    "**Steps**:\n",
    "1. Formula: Simple average of all metrics defined in Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Data Quality Score (%):\n",
      "75.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the overall data quality score as the simple average of completeness, validity, and uniqueness metrics\n",
    "# Normalize uniqueness to a percentage scale for consistency\n",
    "normalized_uniqueness = (uniqueness / len(df)) * 100\n",
    "\n",
    "# Calculate the overall data quality score\n",
    "data_quality_score = (completeness.mean() + validity + normalized_uniqueness) / 3\n",
    "\n",
    "# Display the overall data quality score\n",
    "print(\"Overall Data Quality Score (%):\")\n",
    "print(data_quality_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Creating Expectations for a CSV\n",
    "**Description**: Develop basic data quality expectations using Great Expectations.\n",
    "\n",
    "**Steps**:\n",
    "1. Expectation Suite\n",
    "2. Define Expectations for Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'great_expectations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mge\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Import Great Expectations\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Convert the existing DataFrame to a Great Expectations DataFrame\u001b[39;00m\n\u001b[1;32m      6\u001b[0m ge_df \u001b[38;5;241m=\u001b[39m ge\u001b[38;5;241m.\u001b[39mfrom_pandas(df)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'great_expectations'"
     ]
    }
   ],
   "source": [
    "import great_expectations as ge\n",
    "\n",
    "# Import Great Expectations\n",
    "\n",
    "# Convert the existing DataFrame to a Great Expectations DataFrame\n",
    "ge_df = ge.from_pandas(df)\n",
    "\n",
    "# Define expectations for completeness\n",
    "ge_df.expect_column_values_to_not_be_null(\"Name\")\n",
    "ge_df.expect_column_values_to_not_be_null(\"Email\")\n",
    "ge_df.expect_column_values_to_not_be_null(\"Age\")\n",
    "\n",
    "# Define expectations for validity\n",
    "ge_df.expect_column_values_to_match_regex(\"Email\", r\".+@.+\\..+\")\n",
    "\n",
    "# Define expectations for uniqueness\n",
    "ge_df.expect_column_values_to_be_unique(\"Email\")\n",
    "\n",
    "# Validate the expectations\n",
    "validation_results = ge_df.validate()\n",
    "\n",
    "# Display the validation results\n",
    "print(\"Validation Results:\")\n",
    "print(validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Running and Validating Expectations\n",
    "**Description**: Run the created expectations and generate an output report.\n",
    "\n",
    "**Steps**:\n",
    "1. Validate\n",
    "2. Generate HTML Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'great_expectations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrender\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrenderer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ValidationResultsPageRenderer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrender\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mview\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DefaultJinjaPageView\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Generate an HTML report for the validation results using Great Expectations\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Render the validation results to HTML\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'great_expectations'"
     ]
    }
   ],
   "source": [
    "from great_expectations.render.renderer import ValidationResultsPageRenderer\n",
    "from great_expectations.render.view import DefaultJinjaPageView\n",
    "\n",
    "# Generate an HTML report for the validation results using Great Expectations\n",
    "\n",
    "# Render the validation results to HTML\n",
    "html_content = DefaultJinjaPageView().render(\n",
    "    ValidationResultsPageRenderer().render(validation_results)\n",
    ")\n",
    "\n",
    "# Save the HTML report to a file\n",
    "with open(\"validation_results_report.html\", \"w\") as report_file:\n",
    "    report_file.write(html_content)\n",
    "\n",
    "print(\"Validation report saved as 'validation_results_report.html'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Automating Data Quality Score Calculation\n",
    "**Description**: Automate the data quality score via a script that integrates with Great\n",
    "Expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality score (75.0%) is below the threshold (80.0%).\n",
      "Consider triggering automated data cleaning scripts.\n"
     ]
    }
   ],
   "source": [
    "# Check if the data quality score meets a predefined threshold\n",
    "threshold = 80.0\n",
    "\n",
    "if data_quality_score < threshold:\n",
    "    print(f\"Data quality score ({data_quality_score}%) is below the threshold ({threshold}%).\")\n",
    "    print(\"Consider triggering automated data cleaning scripts.\")\n",
    "else:\n",
    "    print(f\"Data quality score ({data_quality_score}%) meets or exceeds the threshold ({threshold}%).\")\n",
    "    print(\"No immediate action required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recalculated Data Quality Score (%):\n",
      "75.0\n"
     ]
    }
   ],
   "source": [
    "# Automate the data quality score calculation and integrate with Great Expectations\n",
    "\n",
    "# Define a function to calculate data quality score\n",
    "def calculate_data_quality_score(df):\n",
    "    # Completeness: Percentage of non-null values for each column\n",
    "    completeness = df.notnull().mean() * 100\n",
    "\n",
    "    # Validity: Percentage of email fields containing '@'\n",
    "    validity = (df['Email'].str.contains('@', na=False).mean()) * 100\n",
    "\n",
    "    # Uniqueness: Count of distinct entries in the Email column\n",
    "    uniqueness = df['Email'].nunique()\n",
    "\n",
    "    # Normalize uniqueness to a percentage scale for consistency\n",
    "    normalized_uniqueness = (uniqueness / len(df)) * 100\n",
    "\n",
    "    # Calculate the overall data quality score\n",
    "    data_quality_score = (completeness.mean() + validity + normalized_uniqueness) / 3\n",
    "\n",
    "    return data_quality_score\n",
    "\n",
    "# Recalculate the data quality score for the existing DataFrame\n",
    "data_quality_score = calculate_data_quality_score(df)\n",
    "\n",
    "# Display the recalculated data quality score\n",
    "print(\"Recalculated Data Quality Score (%):\")\n",
    "print(data_quality_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Leveraging Data Quality Metrics for Automated Data Cleaning\n",
    "**Description**: Implement a system where if data quality metrics fall below a threshold,\n",
    "automated data cleaning scripts are triggered.\n",
    "\n",
    "**Steps**:\n",
    "1. Define Cleaning Logic\n",
    "2. Integrate with Great Expectations:\n",
    "    - Use an action within the Great Expectations action list that only triggers if quality score is below a threshold, automating the cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns meet the completeness threshold.\n"
     ]
    }
   ],
   "source": [
    "# Check if any column's completeness is below a certain threshold\n",
    "completeness_threshold = 70.0\n",
    "\n",
    "columns_below_threshold = completeness[completeness < completeness_threshold]\n",
    "\n",
    "if not columns_below_threshold.empty:\n",
    "    print(\"The following columns have completeness below the threshold:\")\n",
    "    print(columns_below_threshold)\n",
    "    print(\"Consider taking corrective actions for these columns.\")\n",
    "else:\n",
    "    print(\"All columns meet the completeness threshold.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
